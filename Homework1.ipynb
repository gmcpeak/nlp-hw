{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install [_Miniconda_](https://docs.conda.io/en/main/miniconda.html) or [_Anaconda_](https://docs.anaconda.com/anaconda/install/index.html)\n",
    "2. Create a new virtual Python environment: <code>conda create -n gwnlp Python=3.10</code>\n",
    "3. Activate your environment (and you'll use this Python environment throughout the course - make sure it is selected as the Python interpreter if you are using an IDE like VS Code): <code>conda activate gwnlp</code>\n",
    "4. Install packages (this will give you pandas, pytorch, fastai, spacy, etc.): <code>conda install -c fastchan fastai</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a (5 points). Normalize all of the raw phone numbers with Python RE module. Find one pattern that works for all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Raw | Normalized |\n",
    "| --- | --- |\n",
    "| 2021213121 | +1 (202) 121-3121 |\n",
    "| 12021213121 | +1 (202) 121-3121 |\n",
    "| +12021213121 | +1 (202) 121-3121 |\n",
    "| 202-121-3121 | +1 (202) 121-3121 |\n",
    "| (202)  121 -   3121 | +1 (202) 121-3121 |\n",
    "| (202)121-3121 | +1 (202) 121-3121 |\n",
    "| 862021213121 | +86 (202) 121-3121 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_func(matches):\n",
    "    matches = ''.join(matches)\n",
    "    out = \"+1 (\" + matches[0:2] + \" \" + matches[3:5] + \"-\" + matches[6:9] \n",
    "        \n",
    "\n",
    "in_list = [\"2021213121\", \"12021213121\", \"+12021213121\", \"202-121-3121\", \"(202) 121 - 3121\", \"(202)121-3121\", \"862021213121\"]\n",
    "output = []\n",
    "for x in in_list:\n",
    "    output.append(replace_func(re.findall(r\"[0-9]\", x)))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b (15 points). Use Python RE module to complete the following tasks, with **one** regex pattern **for each**. Show your test samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add spaces around / and #. E.g., \"good/bad\" -> \"good / bad\".\n",
    "2. Replace tokens in ALL CAPS by their lower version. E.g., \"This is AMAZING!\" -> \"This is amazing!\".\n",
    "3. Convert _camel case_ to _snake case_. E.g., \"getNamesFromUserInput\" -> \"get_names_from_user_input\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one():\n",
    "    cases = [\"He / him\", \"He/him\", \"He/ him\", \"He /him\", \"He him\", \"He  /  him\", \"He # him\", \"He#him\", \"He# him\", \"He #him\", \"He him\", \"He  #  him\"]\n",
    "    output = []\n",
    "    for x in cases:\n",
    "        output.append(re.sub(r\"( *(/|#) *)\", r\" \\2 \", x))\n",
    "    print(output)\n",
    "    \n",
    "def replace_2(match_object):\n",
    "    return match_object.group(1).lower()\n",
    "    \n",
    "def two():\n",
    "    cases = [\"MY FUNNY VALENTINE\", \"MY Funny valentine\", \"my FUNNY Valentine\", \"mY FuNnNNnny VALENTINE\"]\n",
    "    output = []\n",
    "    for x in cases:\n",
    "        output.append(re.sub(r\"((^|[^a-z])[A-Z][A-Z]+)\", replace_2, x))\n",
    "    print(output)\n",
    "\n",
    "def replace_3(match_object):\n",
    "    return match_object.group(1)+'_'+match_object.group(2).lower()\n",
    "    \n",
    "def three():\n",
    "    cases = [\"myFunnyValentine\", \"temporaryVariable\", \"javascriptFunction\", \"already_in_the_right_format\", \"almost_but_notQuite\"]\n",
    "    output = []\n",
    "    for x in cases:\n",
    "        output.append(re.sub(r\"([a-z])([A-Z])\", replace_3, x))\n",
    "    print(output)\n",
    "    \n",
    "one()\n",
    "two()\n",
    "three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: For Problem 2 - 5 we will work on a sample of IMDB Reviews dataset. Load the data into a _pandas_ _Dataframe_ (review [the basics of pandas](https://pandas.pydata.org/docs/user_guide/10min.html) if you are new to it) using the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastai.data.external import URLs, untar_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), sum(df['is_valid'] == False), sum(df['is_valid'] == True), sum(df['label'] == 'positive'), sum(df['label'] == 'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a (5 points). \n",
    "- Find at least one thing that needs to be cleaned with regex in the texts. Show your Python code.\n",
    "- Create train/valid split using the column 'is_valid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# One of the most obvious things that needs to be fixed is the presence of the <br/> HTML tags\n",
    "# Let's fix that here\n",
    "\n",
    "for i in range(len(df.text)):\n",
    "    df.text[i] = re.sub(r\"\\<br \\/\\>\", \"\", df.text[i])\n",
    "\n",
    "# Now let's make 20% of the entries a member of the validation class\n",
    "num_entries = len(df.is_valid)\n",
    "selection = np.arange(num_entries)\n",
    "np.random.shuffle(selection)\n",
    "selection = selection[int(num_entries*.8):]\n",
    "df.loc[selection, \"is_valid\"] = True\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_df = df.copy() # saving a copy o the original df for future problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b (5 points). \n",
    "- Implement your own tokenizer for the texts. Requirements: split by space, remove most punctuations and split common abbreviations (e.g., \"don't\" -> \"do\" \"n't\", \"you'll\" -> \"you\" \"'ll\"). \n",
    "- Create 3 vocabularies using top 1000, 5000, and 10000 tokens, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacements(x):\n",
    "    def replace_uppercase(match_object): return match_object.group(0).lower()\n",
    "    x = re.sub(r\"[A-Z]\", replace_uppercase, x)\n",
    "    x = re.sub(r\"[.,\\/#!$%\\^&\\*;:{}=\\-_`~()\\\"\\?]\", \" \", x)\n",
    "    x = re.sub(r\"(n\\'t)\", \" not\", x)\n",
    "    x = re.sub(r\"(\\'ve)\", \" have\", x)\n",
    "    x = re.sub(r\"(\\won't)\", \" would not\", x)\n",
    "    x = re.sub(r\"(\\'ll)\", \" will\", x)\n",
    "    x = re.sub(r\"(\\'re)\", \" are\", x)\n",
    "    x = re.sub(r\"(\\'s)\", \" is\", x)\n",
    "    x = re.sub(r\"(\\'d)\", \" would\", x)\n",
    "    x = re.sub(r\"(\\'m)\", \" am\", x)\n",
    "    x = re.sub(r\"[']\", \"\", x)\n",
    "    x = re.sub(r\"  \", \" \", x)\n",
    "    x = re.sub(r\"\\[\", \"\", x)\n",
    "    x = re.sub(r\"\\]\", \"\", x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "df.text = df.text.apply(lambda x: replacements(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_master_vocab(text):    \n",
    "    master_vocab = []\n",
    "    freqs = []\n",
    "    for x in text:\n",
    "        for word in x.split(' '):\n",
    "            if not(word in master_vocab):\n",
    "                master_vocab.append(word)\n",
    "                freqs.append(1)\n",
    "            else:\n",
    "                idx = master_vocab.index(word)\n",
    "                freqs[idx] = freqs[idx]+1\n",
    "    master_vocab = [x for _, x in sorted(zip(freqs, master_vocab))]\n",
    "    master_vocab.reverse()\n",
    "    return master_vocab\n",
    "\n",
    "master_vocab = build_master_vocab(df.text)\n",
    "vocab_one = master_vocab[:1000]\n",
    "vocab_two = master_vocab[:5000]\n",
    "vocab_three = master_vocab[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c (5 points). \n",
    "- Implement on your own and train a Naive Bayes sentiment classifier in the _training set_. Requirements: use log scales and add-one smoothing.\n",
    "- Report your model performances on the _validation set_, with the 3 vocabs your created in 2b, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpuses_text():\n",
    "    pos_corpus = (''.join(df.loc[(df.label == 'positive') & (df.is_valid == False)].text).split(' '))\n",
    "    neg_corpus = (''.join(df.loc[(df.label == 'negative') & (df.is_valid == False)].text).split(' '))\n",
    "    return pos_corpus, neg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Category ratio\n",
    "# We're using the same dataset here, so these values won't change\n",
    "neg_count = df.label.value_counts()[0]\n",
    "pos_count = df.label.value_counts()[1]\n",
    "total = neg_count + pos_count\n",
    "overall_pos_doc_freq = math.log(pos_count/total)\n",
    "overall_neg_doc_freq = math.log(neg_count/total)\n",
    "def naive_bayes_train(vocab, mode):\n",
    "    pos_corpus = None\n",
    "    neg_corpus = None\n",
    "    if mode == 'text':\n",
    "        pos_corpus, neg_corpus = get_corpuses_text()\n",
    "    elif mode == 'lemma':\n",
    "        pos_corpus, neg_corpus = get_corpuses_lemma()\n",
    "    # Calc frequencies of word in each class\n",
    "    pos_freqs = [None]*len(vocab)\n",
    "    neg_freqs = [None]*len(vocab)\n",
    "    for i in range(len(vocab)):\n",
    "        pos_freqs[i] = math.log((pos_corpus.count(vocab[i])+1)/(len(pos_corpus)+len(vocab)))\n",
    "        neg_freqs[i] = math.log((neg_corpus.count(vocab[i])+1)/(len(neg_corpus)+len(vocab)))\n",
    "    return pos_freqs, neg_freqs\n",
    "        \n",
    "pos_1, neg_1 = naive_bayes_train(vocab_one, 'text')\n",
    "pos_2, neg_2 = naive_bayes_train(vocab_two, 'text')\n",
    "pos_3, neg_3 = naive_bayes_train(vocab_three, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_val_accuracy(predictions, labels):\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == labels[i]:\n",
    "            correct = correct + 1\n",
    "    return (correct/len(labels))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_validate(pos_scores, neg_scores, val_set, vocab):\n",
    "    classifications = [\"negative\"]*len(val_set.text)\n",
    "    for i in range(len(val_set.text)):\n",
    "        points_p = overall_pos_doc_freq\n",
    "        points_n = overall_neg_doc_freq\n",
    "        for word in val_set.text[i].split(' '):\n",
    "            if word in vocab:\n",
    "                idx = vocab.index(word)\n",
    "                points_p = points_p + pos_scores[idx]\n",
    "                points_n = points_n + neg_scores[idx]\n",
    "        if points_p > points_n:\n",
    "            classifications[i] = \"positive\"\n",
    "    return classifications, calc_val_accuracy(classifications, val_set.label)\n",
    "\n",
    "v1_predictions, v1_accuracy = naive_bayes_validate(pos_1, neg_1, df.loc[df.is_valid == True].reset_index(drop=True), vocab_one)\n",
    "v2_predictions, v2_accuracy = naive_bayes_validate(pos_2, neg_2, df.loc[df.is_valid == True].reset_index(drop=True), vocab_two)\n",
    "v3_predictions, v3_accuracy = naive_bayes_validate(pos_3, neg_3, df.loc[df.is_valid == True].reset_index(drop=True), vocab_three)\n",
    "print(\"Accuracy for vocab of size 1000: \"+str(v1_accuracy))\n",
    "print(\"Accuracy for vocab of size 5000: \"+str(v2_accuracy))\n",
    "print(\"Accuracy for vocab of size 10000: \"+str(v3_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d (5 points). Use [_spaCy_](https://spacy.io/) to _tokenize_ and _lemmatize_ this time. Get a new vocab of top 10000 lemmas. Retrain your model on this vocab and report its performance on the validation set.\n",
    "(Note that spaCy relies on language-specific databases to work. Even though it is already importable, you still need to install its dependency for English. If you are in your _jupyter notebook_, create a new cell and execute the following: <code>!python -m spacy download en_core_web_sm</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = og_df.copy()\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_spacy(x): return [token.text for token in x if not token.is_stop]\n",
    "def get_lemmas_spacy(x): return [token.lemma_ for token in x if not token.is_stop]\n",
    "\n",
    "df['token'] = df.text.apply(lambda x: remove_stop_spacy(nlp(x)))\n",
    "df['lemma'] = df.text.apply(lambda x: get_lemmas_spacy(nlp(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lemma_vocab(text):    \n",
    "    master_vocab = []\n",
    "    freqs = []\n",
    "    for x in text:\n",
    "        for word in x:\n",
    "            if not(word in master_vocab):\n",
    "                master_vocab.append(word)\n",
    "                freqs.append(1)\n",
    "            else:\n",
    "                idx = master_vocab.index(word)\n",
    "                freqs[idx] = freqs[idx]+1\n",
    "    master_vocab = [x for _, x in sorted(zip(freqs, master_vocab))]\n",
    "    master_vocab.reverse()\n",
    "    return master_vocab\n",
    "\n",
    "lemma_vocab = build_lemma_vocab(df.lemma)[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpuses_lemma():\n",
    "    pos_corpus = (df.loc[(df.label == 'positive') & (df.is_valid == False)].lemma).sum()\n",
    "    neg_corpus = (df.loc[(df.label == 'negative') & (df.is_valid == False)].lemma).sum()\n",
    "    return pos_corpus, neg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_4, neg_4 = naive_bayes_train(lemma_vocab, 'lemma')\n",
    "v4_predictions, v4_accuracy = naive_bayes_validate(pos_4, neg_4, df.loc[df.is_valid == True].reset_index(drop=True), lemma_vocab)\n",
    "print(\"Accuracy for vocab of spaCy lemmas: \"+str(v4_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a (10 points). \n",
    "- Implement your own _subword tokenizer_ (the algorithm can be found in the slides). \n",
    "- Create 3 vocabularies of size 1000, 5000, and 10000, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "df = og_df.copy()\n",
    "df.text = df.text.apply(lambda x: replacements(x))\n",
    "def most_common_pair(a, tokens):\n",
    "    occurences = defaultdict(int)\n",
    "    for i in range(len(tokens)-1):\n",
    "        occurences[(tokens[i], tokens[i+1])] += 1\n",
    "    sorted_occurences = {k: v for k, v in sorted(occurences.items(), key=lambda item: item[1], reverse=True)}\n",
    "    for key, val in sorted_occurences.items():\n",
    "        return ''.join(key)\n",
    "\n",
    "def bpe(c, k):\n",
    "    c = [i for i in c if i != ' ']\n",
    "    v = list(set(c))\n",
    "    while len(v) < k:\n",
    "        winner = most_common_pair(v, c)\n",
    "        v.append(winner)\n",
    "        for j in range(len(c)-1):\n",
    "            if (c[j]+c[j+1]) in v:\n",
    "                c[j] = c[j]+c[j+1]\n",
    "                c[j+1] = \"DROP\"\n",
    "        c = [i for i in c if i != 'DROP']\n",
    "    return v, c\n",
    "\n",
    "bpe_vocab1, c1 = bpe([*''.join(df.text)], 1000)\n",
    "bpe_vocab2, c2 = bpe([*''.join(df.text)], 5000)\n",
    "bpe_vocab3, c3 = bpe([*''.join(df.text)], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b (5 points). Compare the number of unknown words in your training set between the 3 tokenizers and 3 subword tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_known_words(known, corp):\n",
    "    for i in range(len(corp)):\n",
    "        if corp[i] in known:\n",
    "            corp[i] = \"DROP\"\n",
    "    return [x for x in corp if ix != 'DROP']\n",
    "\n",
    "corpus = og_df.copy().text.apply(lambda x: replacements(x))\n",
    "\n",
    "print(\"Unknown words in 1000 word vocab set: \" + len(remove_known_words(vocab_one, corpus.copy())))\n",
    "print(\"Unknown words in 5000 word vocab set: \" + len(remove_known_words(vocab_two, corpus.copy())))\n",
    "print(\"Unknown words in 10000 word vocab set: \" + len(remove_known_words(vocab_three, corpus.copy())))\n",
    "\n",
    "print(\"Unknown words in 1000 SUBword vocab set: \" + len(remove_known_words(bpe_vocab1, corpus.copy())))\n",
    "print(\"Unknown words in 5000 SUBword vocab set: \" + len(remove_known_words(bpe_vocab2, corpus.copy())))\n",
    "print(\"Unknown words in 10000 SUBword vocab set: \" + len(remove_known_words(bpe_vocab3, corpus.copy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c (5 points). Train your Naive Bayes classifier with the subword tokenizer of 10000 tokens. Compare your model performance (better/worse/same?) and give your analysis (why)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_5, neg_5 = naive_bayes_train(bpe_vocab3, 'text')\n",
    "v5_predictions, v5_accuracy = naive_bayes_validate(pos_5, neg_5, df.loc[df.is_valid == True].reset_index(drop=True), bpe_vocab3)\n",
    "print(\"Accuracy for vocab of 10000 bpe subwords: \"+str(v5_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a (10 points). Build two probabilistic language models using 2-gram and 3-gram, respectively, on the _entire_ texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = og_df.copy()\n",
    "def get_ngrams(n, grams):\n",
    "    ngrams = defaultdict(int)\n",
    "    for i in range(len(grams)-(n)):\n",
    "        ngrams[' '.join(grams[i:(i+n)])] += 1\n",
    "    return list(ngrams.keys())\n",
    "\n",
    "def calc_ngram_prob(ngram, text, n):\n",
    "    temp_ngram = ' '.join(ngram)\n",
    "    prob = text.count(temp_ngram)\n",
    "    prob = (prob)/(text.count(' '.join(re.findall(r\"[a-zA-Z]+\", temp_ngram)[:n-1])))\n",
    "    if prob != 0:\n",
    "        prob = math.log(prob)\n",
    "    return prob\n",
    "\n",
    "def get_ngram_probs(ngrams, text, n):\n",
    "    probabilities = [0]*len(ngrams)\n",
    "    running_total = 0\n",
    "    for i in range(len(ngrams)):\n",
    "        probabilities[i] = calc_ngram_prob(ngrams[i], text, n)\n",
    "    return probabilities\n",
    "\n",
    "def find_sentence_probs(n, in_sentences):\n",
    "    text = ''.join(df.text.apply(lambda x: replacements(x)))\n",
    "    words = re.findall(r\"[a-zA-Z]+\", text)\n",
    "    ngrams = get_ngrams(n, words)\n",
    "    ng_probs = get_ngram_probs(ngrams, text, n)\n",
    "    out_probs = []\n",
    "    for in_sen in in_sentences:\n",
    "        in_sen_ngrams = []\n",
    "        in_sen_ngrams.append(get_ngrams(n, in_sen.lower().split(' ')))\n",
    "        in_sen_probs = get_ngram_probs(in_sen_ngrams, text, n)\n",
    "        curr_prob = 0\n",
    "        for x in in_sen_probs:\n",
    "            curr_prob += x\n",
    "        out_probs.append(curr_prob)\n",
    "    return out_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b (10 points). Generate 5 examples for each of the LM. Compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = ['This movie was great', 'Hard to believe she was the producer on this dog', 'I was so bored', 'It had very bad acting', 'It is my new favorite film']\n",
    "print('Log probabilities for 2-gram calculation:')\n",
    "print(find_sentence_probs(2, test_sentences))\n",
    "    \n",
    "print('Log probabilities for 3-gram calculation:')\n",
    "print(find_sentence_probs(2, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a (10 points). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run topic modeling with SVD for 2, 6, and 10 topics, respectively.\n",
    "- Extract 10 keywords for each topic.\n",
    "- Try to mannually assign topic labels for (some of) them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "topic_word_list = []\n",
    "def get_topics(components, vocab, num_topics): \n",
    "    for i, comp in enumerate(components):\n",
    "        terms_comp = zip(vocab,comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "        temp = \"\"\n",
    "        for t in sorted_terms:\n",
    "            topic = t[0]\n",
    "            temp = temp + \" \" + str(topic)\n",
    "        topic_word_list.append(temp)\n",
    "    print(topic_word_list)\n",
    "    return topic_word_list\n",
    "\n",
    "def model_topics(num_topics):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',smooth_idf=True) \n",
    "    in_matrix = vectorizer.fit_transform(og_df.copy().text.apply(lambda x: replacements(x))).todense()\n",
    "    svd_modeling= TruncatedSVD(n_components=num_topics, algorithm='randomized', n_iter=200, random_state=42)\n",
    "    svd_modeling.fit(in_matrix)\n",
    "    components=svd_modeling.components_\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    get_topics(components, vocab, num_topics)\n",
    "    \n",
    "for i in [2, 6, 10]:\n",
    "    model_topics(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the entries when running for 10 topics, specifically:\n",
    "*' action great series bourne movie horror movies films original fun'*, \n",
    "*' movies watch love great comedy bad actors people funny life'*, \n",
    "Seem to respectively be about:\n",
    "Thriller movies such as action, horror, and similar genres as well as:\n",
    "Fun comedy movies where the acting isn't great but perhaps still enjoyable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b (5 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the following:\n",
    "- Remove stopwords\n",
    "- Lemmatize\n",
    "- Keep only nouns, verbs, and adjs with the help of spaCy POS tagger\n",
    "- Remove certain named entities (choose whatever makes sense to you)\n",
    "- Remove html tags\n",
    "- Remove non-ascii characters\n",
    "\n",
    "And run SVD again for 10 topics. Compare your results with 5a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c (5 points). Find 2 most similar pairs of reviews using document embeddings derived from SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "36e2a24421450a9a7584f55625e393e93224ff98b3fe735792f2ff14e7a7b14a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
